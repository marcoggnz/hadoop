# Análisis del Mercado de Películas Actual Mediante Hadoop

Este proyecto pone en práctica los conceptos y nociones básicas de Hadoop mediante tres tareas, dos de ellas teóricas y una práctica. El objetivo es realizar un análisis del mercado actual de películas para una startup y determinar insights aprovechando un dataset público de [MovieLens](https://grouplens.org/datasets/movielens/). Este proyecto es parte del Máster en Big Data Architecture & Engineering de Datahack.

## Tecnologías utilizadas
* Hadoop
* Hive
* Sqoop
* MySQL

## Introducción

En la actualidad, la extracción de información y el análisis de datos son fundamentales para la toma de decisiones en los negocios. Sin embargo, la cantidad de datos generados cada día es enorme, lo que hace que el procesamiento y análisis de estos datos sea una tarea difícil y costosa. Hadoop es una tecnología que ha revolucionado la forma en que las empresas extraen información y analizan datos.

Hadoop es un framework de procesamiento distribuido de código abierto que permite el almacenamiento y procesamiento de grandes cantidades de datos en clusters de computadoras. Con Hadoop, es posible procesar grandes conjuntos de datos en paralelo, lo que reduce el tiempo de procesamiento y el costo. Además, Hadoop utiliza un sistema de archivos distribuidos que permite el almacenamiento de datos de manera eficiente.

Uno de los casos de uso más populares de Hadoop es el procesamiento de datos de usuario para implementar sistemas de recomendación en empresas como Netflix y Spotify. Con Hadoop, es posible procesar de manera eficiente los gustos de los usuarios, lo que permite a estas empresas proporcionar recomendaciones precisas y personalizadas a sus usuarios…

## Planteamiento de Negocio

El objetivo principal de este proyecto es crear el departamento de analítica desde cero. Este departamento tiene como función principal la extracción y análisis de los datos necesarios para generar insights que permitan a la mesa ejecutiva la definición de la estrategia de la startup.
El proyecto consiste en tres tareas. El desarrollo de cada una de ellas se encuentra detallado a continuación en los siguientes archivos:
* [Tarea 1](tarea1.md)
* [Tarea 2](tarea2.md)
* [Tarea 3](tarea3.md)

> **Note:** Este proyecto ha sido desarrollado en una máquina virtual de Cloudewra de forma local, tratandose de un cluster preconfigurado.

## Referencias

* https://www.databricks.com/glossary/hadoop-distributed-file-system-hdfs
* https://www.projectpro.io/article/impala-vs-hive-difference-between-sql-on-hadoop-components/180
* https://phoenix.apache.org/
* *Understanding ETL: Data Pipelines for Modern Data Architectures* by O’Reilly
